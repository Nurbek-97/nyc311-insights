{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19cba4f",
   "metadata": {},
   "source": [
    "# Creating required submission and anomaly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote submission.csv ((93, 2)) and anomalies.csv ((0, 5)). All checks passed.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, numpy as np, pandas as pd\n",
    "\n",
    "# ---- Config (matches challenge spec) ----\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "SUB_PATH  = \"submission.csv\"\n",
    "ANOM_PATH = \"anomalies.csv\"\n",
    "REQUIRED_SUB_COLS  = [\"Date\", \"Predicted_Total_Calls\"]\n",
    "REQUIRED_ANOM_COLS = [\"Date\",\"Actual\",\"Expected\",\"Anomaly_Score\",\"Note\"]\n",
    "\n",
    "def _assert_columns(df, cols, name):\n",
    "    if list(df.columns) != cols:\n",
    "        raise AssertionError(f\"{name} columns must be {cols} (got {list(df.columns)})\")\n",
    "\n",
    "def _assert_date_format(series, name):\n",
    "    try:\n",
    "        pd.to_datetime(series, format=\"%Y-%m-%d\", errors=\"raise\")\n",
    "    except Exception as e:\n",
    "        raise AssertionError(f\"{name} has invalid date format (expect YYYY-MM-DD): {e}\")\n",
    "\n",
    "# optional local train (not required for reproducibility)\n",
    "train = None\n",
    "if os.path.exists(\"data/train.csv\"):\n",
    "    train = pd.read_csv(\"data/train.csv\", parse_dates=[\"date\"])\n",
    "    if \"total_calls\" not in train.columns:\n",
    "        raise AssertionError(\"data/train.csv must include columns: date,total_calls\")\n",
    "\n",
    "# Use fixed scoring window per brief\n",
    "START, END = \"2025-05-01\", \"2025-08-01\"\n",
    "future_dates = pd.date_range(START, END, freq=\"D\")\n",
    "\n",
    "# Constant forecast (deterministic); replaced later by the real model\n",
    "const_val = float(train[\"total_calls\"].iloc[-1]) if train is not None else 10000.0\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"Date\": future_dates.strftime(\"%Y-%m-%d\"),\n",
    "    \"Predicted_Total_Calls\": const_val\n",
    "})[REQUIRED_SUB_COLS]\n",
    "_assert_columns(submission, REQUIRED_SUB_COLS, \"submission.csv\")\n",
    "_assert_date_format(submission[\"Date\"], \"submission.Date\")\n",
    "submission.to_csv(SUB_PATH, index=False)\n",
    "\n",
    "# anomalies placeholder with correct header\n",
    "anomalies = pd.DataFrame(columns=REQUIRED_ANOM_COLS)\n",
    "_assert_columns(anomalies, REQUIRED_ANOM_COLS, \"anomalies.csv\")\n",
    "anomalies.to_csv(ANOM_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Wrote {SUB_PATH} ({submission.shape}) and {ANOM_PATH} ({anomalies.shape}). All checks passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d6f25",
   "metadata": {},
   "source": [
    "# Fetch & aggregate NYC 311 data into daily train/test CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4486aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved data/train.csv and data/test.csv\n",
      "Train: 2024-08-01 → 2025-04-30 | rows: 273\n",
      "Test : 2025-05-01 → 2025-08-01 | rows: 93\n"
     ]
    }
   ],
   "source": [
    "# Windows per challenge (training/features) and (forecast/scoring)\n",
    "TRAIN_START, TRAIN_END = \"2024-08-01\", \"2025-04-30\"\n",
    "TEST_START,  TEST_END  = \"2025-05-01\", \"2025-08-01\"\n",
    "\n",
    "import time, requests, pandas as pd, numpy as np, os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "BASE = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json\"\n",
    "\n",
    "def fetch_daily_counts(start_ymd: str, end_ymd: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust SoQL aggregation by calendar date with retries & paging.\n",
    "    Returns columns: date,total_calls,manhattan,bronx,brooklyn,queens,staten_island\n",
    "    \"\"\"\n",
    "    select = (\n",
    "        \"date_trunc_ymd(created_date) as date,\"\n",
    "        \"count(1) as total_calls,\"\n",
    "        \"sum(case when borough='MANHATTAN' then 1 else 0 end) as manhattan,\"\n",
    "        \"sum(case when borough='BRONX' then 1 else 0 end) as bronx,\"\n",
    "        \"sum(case when borough='BROOKLYN' then 1 else 0 end) as brooklyn,\"\n",
    "        \"sum(case when borough='QUEENS' then 1 else 0 end) as queens,\"\n",
    "        \"sum(case when borough='STATEN ISLAND' then 1 else 0 end) as staten_island\"\n",
    "    )\n",
    "    limit, offset, frames = 50000, 0, []\n",
    "    while True:\n",
    "        params = {\n",
    "            \"$select\": select,\n",
    "            \"$where\": f\"created_date between '{start_ymd}T00:00:00' and '{end_ymd}T23:59:59'\",\n",
    "            \"$group\": \"date_trunc_ymd(created_date)\",\n",
    "            \"$order\": \"date_trunc_ymd(created_date)\",\n",
    "            \"$limit\": limit, \"$offset\": offset\n",
    "        }\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                r = requests.get(BASE, params=params, timeout=120)\n",
    "                r.raise_for_status()\n",
    "                chunk = pd.DataFrame(r.json())\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == 2:\n",
    "                    raise\n",
    "                print(f\"⚠️ Retry {attempt+1}/3 offset={offset} due to: {e}\")\n",
    "                time.sleep(5)\n",
    "        if chunk.empty: break\n",
    "        frames.append(chunk)\n",
    "        if len(chunk) < limit: break\n",
    "        offset += limit\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"API returned no rows. Check dates/network.\")\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    for c in [\"total_calls\",\"manhattan\",\"bronx\",\"brooklyn\",\"queens\",\"staten_island\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # contiguity check (no missing calendar days)\n",
    "    full = pd.date_range(start_ymd, end_ymd, freq=\"D\").date\n",
    "    have = df[\"date\"].tolist()\n",
    "    missing = sorted(set(full) - set(have))\n",
    "    if missing:\n",
    "        raise AssertionError(f\"Missing days: {missing[:5]} ... total {len(missing)}\")\n",
    "    return df\n",
    "\n",
    "train_df = fetch_daily_counts(TRAIN_START, TRAIN_END)\n",
    "test_df  = fetch_daily_counts(TEST_START,  TEST_END)\n",
    "\n",
    "train_df.to_csv(\"data/train.csv\", index=False)\n",
    "test_df.to_csv(\"data/test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved data/train.csv and data/test.csv\")\n",
    "print(\"Train:\", train_df[\"date\"].min(), \"→\", train_df[\"date\"].max(), \"| rows:\", len(train_df))\n",
    "print(\"Test :\", test_df[\"date\"].min(),  \"→\", test_df[\"date\"].max(),  \"| rows:\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa7d91",
   "metadata": {},
   "source": [
    "#  Leak-safe features, train XGBoost, predict scoring window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "579c0298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 273 2024-08-01 → 2025-04-30\n",
      "Val RMSE≈553 | MAPE≈4.93% on last 28d\n",
      "✅ submission.csv written: (93, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, os, xgboost as xgb\n",
    "\n",
    "# ---- windows (must match brief) ----\n",
    "TRAIN_START, TRAIN_END = \"2024-08-01\", \"2025-04-30\"\n",
    "TEST_START,  TEST_END  = \"2025-05-01\", \"2025-08-01\"\n",
    "\n",
    "# ---- load daily aggregates built in Step 5 (if missing, raise) ----\n",
    "assert os.path.exists(\"data/train.csv\"), \"Missing data/train.csv. Run Step 5 first.\"\n",
    "train_df = pd.read_csv(\"data/train.csv\", parse_dates=[\"date\"])\n",
    "print(\"Train rows:\", len(train_df), train_df[\"date\"].min().date(), \"→\", train_df[\"date\"].max().date())\n",
    "\n",
    "# ---- Feature builder (robust to Series or DatetimeIndex) ----\n",
    "def make_calendar_features(dates) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Accepts pd.Series, list-like, or DatetimeIndex.\n",
    "    Returns a DataFrame with calendar & cyclic features (no target lags).\n",
    "    \"\"\"\n",
    "    dt = pd.to_datetime(dates)\n",
    "    # Ensure we have a Series so .dt works even for DatetimeIndex\n",
    "    if isinstance(dt, pd.DatetimeIndex):\n",
    "        dt = pd.Series(dt)\n",
    "    df = pd.DataFrame({\"date\": dt.dt.date})\n",
    "    df[\"dow\"] = dt.dt.dayofweek                  # 0-6\n",
    "    df[\"is_weekend\"] = (df[\"dow\"] >= 5).astype(int)\n",
    "    df[\"dayofyear\"] = dt.dt.dayofyear\n",
    "    # .isocalendar() returns a frame with 'week' (pandas>=1.1)\n",
    "    df[\"weekofyear\"] = dt.dt.isocalendar().week.astype(int)\n",
    "    df[\"month\"] = dt.dt.month\n",
    "    # cyclic encodings\n",
    "    df[\"doy_sin\"] = np.sin(2*np.pi*df[\"dayofyear\"]/366)\n",
    "    df[\"doy_cos\"] = np.cos(2*np.pi*df[\"dayofyear\"]/366)\n",
    "    df[\"mon_sin\"] = np.sin(2*np.pi*df[\"month\"]/12)\n",
    "    df[\"mon_cos\"] = np.cos(2*np.pi*df[\"month\"]/12)\n",
    "    df[\"woy_sin\"] = np.sin(2*np.pi*df[\"weekofyear\"]/53)\n",
    "    df[\"woy_cos\"] = np.cos(2*np.pi*df[\"weekofyear\"]/53)\n",
    "    # linear trend from train start\n",
    "    df[\"t\"] = (pd.to_datetime(df[\"date\"]) - pd.to_datetime(TRAIN_START)).dt.days\n",
    "    return df\n",
    "\n",
    "# ---- build training matrix ----\n",
    "train_feat = make_calendar_features(train_df[\"date\"])\n",
    "train_feat[\"y\"] = train_df[\"total_calls\"].astype(float)\n",
    "\n",
    "FEATURES = [c for c in train_feat.columns if c not in [\"date\",\"y\"]]\n",
    "Xtr, ytr = train_feat[FEATURES], train_feat[\"y\"]\n",
    "\n",
    "# ---- quick internal validation (last 28 days) ----\n",
    "split_idx = len(train_feat) - 28\n",
    "X_fit, y_fit = Xtr.iloc[:split_idx], ytr.iloc[:split_idx]\n",
    "X_val, y_val = Xtr.iloc[split_idx:], ytr.iloc[split_idx:]\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=400, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.9, colsample_bytree=0.9, random_state=42, n_jobs=0\n",
    ")\n",
    "model.fit(X_fit, y_fit)\n",
    "val_pred = model.predict(X_val)\n",
    "rmse = np.sqrt(np.mean((val_pred - y_val.values)**2))\n",
    "mape = np.mean(np.abs((y_val.values - val_pred) / np.clip(y_val.values, 1e-6, None))) * 100\n",
    "print(f\"Val RMSE≈{rmse:,.0f} | MAPE≈{mape:.2f}% on last 28d\")\n",
    "\n",
    "# ---- refit on all train data ----\n",
    "model.fit(Xtr, ytr)\n",
    "\n",
    "# ---- build scoring features (dates only; no leakage) ----\n",
    "future_dates = pd.date_range(TEST_START, TEST_END, freq=\"D\")\n",
    "Xte = make_calendar_features(future_dates)\n",
    "\n",
    "# ---- weekday baseline (strong simple prior) ----\n",
    "dow_mean = train_feat.groupby(\"dow\")[\"y\"].mean()\n",
    "pred_dow = Xte[\"dow\"].map(dow_mean).values\n",
    "\n",
    "# ---- ML prediction + robust blend ----\n",
    "pred_ml = model.predict(Xte[[c for c in FEATURES]])\n",
    "pred = 0.7*pred_ml + 0.3*pred_dow\n",
    "\n",
    "# ---- write submission.csv (exact schema) ----\n",
    "submission = pd.DataFrame({\n",
    "    \"Date\": future_dates.strftime(\"%Y-%m-%d\"),\n",
    "    \"Predicted_Total_Calls\": pred\n",
    "})[[\"Date\",\"Predicted_Total_Calls\"]]\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv written:\", submission.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308f92b",
   "metadata": {},
   "source": [
    "# Compute anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5ddde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ anomalies.csv written with top-5 spikes & dips: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os, numpy as np\n",
    "\n",
    "ANOM_COLS = [\"Date\",\"Actual\",\"Expected\",\"Anomaly_Score\",\"Note\"]\n",
    "dates = pd.read_csv(\"submission.csv\")[\"Date\"]\n",
    "expected = pd.read_csv(\"submission.csv\")[\"Predicted_Total_Calls\"]\n",
    "\n",
    "if os.path.exists(\"data/test.csv\"):\n",
    "    test_df = pd.read_csv(\"data/test.csv\", parse_dates=[\"date\"])\n",
    "    if \"total_calls\" in test_df.columns:\n",
    "        actual_map = test_df.set_index(test_df[\"date\"].dt.strftime(\"%Y-%m-%d\"))[\"total_calls\"]\n",
    "        actual = dates.map(actual_map).astype(\"float\")\n",
    "        df = pd.DataFrame({\n",
    "            \"Date\": dates,\n",
    "            \"Actual\": actual.round().astype(\"Int64\"),\n",
    "            \"Expected\": expected.astype(float)\n",
    "        })\n",
    "        df[\"Anomaly_Score\"] = (df[\"Actual\"] - df[\"Expected\"]) / df[\"Expected\"].clip(lower=1e-6)\n",
    "        df[\"Note\"] = \"\"\n",
    "        # select top-5 spikes & top-5 dips for readability (still fine to output all)\n",
    "        spikes = df.nlargest(5, \"Anomaly_Score\")\n",
    "        dips   = df.nsmallest(5, \"Anomaly_Score\")\n",
    "        out = pd.concat([spikes, dips]).sort_values(\"Date\")\n",
    "        out = out[ANOM_COLS]\n",
    "        out.to_csv(\"anomalies.csv\", index=False)\n",
    "        print(\"✅ anomalies.csv written with top-5 spikes & dips:\", out.shape)\n",
    "    else:\n",
    "        pd.DataFrame(columns=ANOM_COLS).to_csv(\"anomalies.csv\", index=False)\n",
    "        print(\"ℹ️ data/test.csv has no 'total_calls'; wrote empty anomalies.csv\")\n",
    "else:\n",
    "    pd.DataFrame(columns=ANOM_COLS).to_csv(\"anomalies.csv\", index=False)\n",
    "    print(\"ℹ️ data/test.csv not found; wrote empty anomalies.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
